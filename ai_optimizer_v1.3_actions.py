# =================================================================================
# AI Optimizer & Hexo Formatter v1.3 (GitHub Actions, Stateful)
# v1.3 æ›´æ–°ï¼šé›†æˆSQLiteæ•°æ®åº“å®ç°æŒä¹…åŒ–ï¼Œç¡®ä¿ä»»åŠ¡å¯ä¸­æ–­å’Œç»­è¡Œã€‚
# =================================================================================
import os, re, json, shutil
from datetime import datetime
from database import initialize_database, add_processed_article, get_processed_articles
try:
    from bs4 import BeautifulSoup
    import html2text
    import google.generativeai as genai
except ImportError as e:
    print(f"[ä¸¥é‡é”™è¯¯] ç¼ºå°‘å¿…è¦çš„åº“: {e.name}\nè¯·è¿è¡Œ: pip install {e.name}")
    exit()

PUSHPLUS_TOKEN = os.getenv('PUSHPLUS_TOKEN')
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
RAW_DATA_PARENT_FOLDER = "South-Plus-Raw-Data"
FINAL_OUTPUT_PARENT_FOLDER = "South-Plus-Articles"
PUSHPLUS_URL = 'http://www.pushplus.plus/send'
REPORTING_BATCH_SIZE = 50

def send_pushplus_notification(title, content):
    if not PUSHPLUS_TOKEN:
        print(f"[é€šçŸ¥] PushPlus Tokenæœªé…ç½®ï¼Œè·³è¿‡å‘é€ã€‚\n--- {title} ---\n{content}\n--------------------")
        return
    try:
        data = {"token": PUSHPLUS_TOKEN, "title": title, "content": content.replace('\n', '<br>'), "template": "html"}
        response = requests.post(PUSHPLUS_URL, json=data)
        if response.json()['code'] == 200:
            print(f"[é€šçŸ¥] æˆåŠŸå‘é€åˆ°PushPlus: {title}")
        else:
            print(f"[é€šçŸ¥é”™è¯¯] PushPluså‘é€å¤±è´¥: {response.text}")
    except Exception as e:
        print(f"[é€šçŸ¥é”™è¯¯] å‘é€PushPlusé€šçŸ¥æ—¶å‡ºç°å¼‚å¸¸: {e}")

def add_alt_tags_to_html(html_content, seo_title):
    soup = BeautifulSoup(html_content, 'html.parser')
    img_tags = soup.find_all('img')
    if not img_tags: return html_content
    for i, img in enumerate(img_tags, 1):
        if not img.get('alt', '').strip():
            img['alt'] = f"{seo_title} - å›¾{i}"
    return str(soup)

def generate_seo_with_ai(model, original_title, content_text_snippet, author):
    if not model: return None
    prompt = f"""ä½ æ˜¯ä¸€åä¸“ä¸šçš„æ¸¸æˆåšå®¢SEOä¸“å®¶ã€‚åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç”ŸæˆSEOå…ƒæ•°æ®ã€‚
**åŸå§‹æ ‡é¢˜:** "{original_title}"
**ä½œè€…:** "{author}"
**å†…å®¹æ‘˜è¦:** "{content_text_snippet}..."
**ä»»åŠ¡:** ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æœ‰é¢å¤–è§£é‡Šã€‚
{{
  "seo_title": "ä¸€ä¸ª50-70å­—ç¬¦çš„å¸å¼•äººçš„SEOæ ‡é¢˜ã€‚",
  "description": "ä¸€æ®µ120-160å­—ç¬¦çš„Metaæè¿°ã€‚",
  "tags": ["5åˆ°8ä¸ªç›¸å…³æ ‡ç­¾çš„æ•°ç»„", "ä¾‹å¦‚ 'è§†è§‰å°è¯´', 'æ‹çˆ±æ¨¡æ‹Ÿ'"]
}}"""
    try:
        response = model.generate_content(prompt, request_options={"timeout": 60})
        cleaned_text = response.text.strip().replace('```json', '').replace('```', '').strip()
        result = json.loads(cleaned_text)
        if 'seo_title' in result and 'description' in result and 'tags' in result:
            return result
        return None
    except Exception as e:
        print(f"      [AIé”™è¯¯] è°ƒç”¨Gemini APIå¤±è´¥: {e.__class__.__name__}")
        return None

def sanitize_filename(filename):
    safe_name = re.sub(r'[\\/*?:"<>|]', "", filename).strip()
    return safe_name[:150]

def create_seo_title(original_title, limit=70):
    cleaned_title = re.sub(r'ã€.*?ã€‘|\[.*?\]', '', original_title)
    cleaned_title = re.sub(r'\s*[\d\.]+G[B|]?.*$', '', cleaned_title, flags=re.IGNORECASE).strip()
    if not cleaned_title: cleaned_title = original_title
    if len(cleaned_title) > limit:
        shortened_at = cleaned_title.rfind(' ', 0, limit - 3)
        return cleaned_title[:shortened_at] + "..." if shortened_at != -1 else cleaned_title[:limit - 3] + "..."
    return cleaned_title

def create_meta_description(html_content, markdown_text, limit=160):
    text = re.sub(r'<.*?>|#+\s*|---\s*|\*\*|__', '', markdown_text)
    text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)
    text = ' '.join(re.sub(r'!\[.*?\]\(.*?\)|{%.*?%}', '', text).split())
    if len(text) > limit:
        text = text[:limit].rsplit(' ', 1)[0] + '...'
    return text.replace('"', 'â€œ')

def extract_tags_from_title(original_title, author):
    tags = re.findall(r'ã€(.*?)ã€‘|\[(.*?)\]', original_title)
    flat_tags = [item for tpl in tags for item in tpl if item]
    blacklist = ['è‡ªè´­', 'æ–°æ±‰åŒ–ä½œå“', 'åˆé›†', 'bd', 'od', 'bo']
    cleaned_tags = {tag.strip() for tag in flat_tags if tag.strip().lower() not in blacklist and not re.match(r'^\d+(\.\d+)?(gb|g|mb)$', tag.strip(), re.IGNORECASE)}
    cleaned_tags.update(["æ±‰åŒ–", "GALGAME"])
    if author != "æœªçŸ¥ä½œè€…": cleaned_tags.add(author)
    return list(cleaned_tags)

def generate_structured_data(seo_title, original_title, cover_url, description, hexo_date, author_name, source_url):
    schema = {"@context": "https://schema.org", "@type": "VideoGame", "name": seo_title, "alternateName": original_title, "description": description, "image": cover_url, "datePublished": hexo_date, "author": {"@type": "Person", "name": author_name}, "operatingSystem": "Windows", "applicationCategory": "GameApplication", "url": source_url}
    schema = {k: v for k, v in schema.items() if v}
    return f'<script type="application/ld+json">\n{json.dumps(schema, ensure_ascii=False, indent=2)}\n</script>'

def get_existing_processed_titles(parent_folder):
    existing_titles = set()
    if not os.path.exists(parent_folder): return existing_titles
    for root, _, files in os.walk(parent_folder):
        for name in files:
            if name.endswith(".md"): existing_titles.add(os.path.splitext(name)[0])
    return existing_titles

def process_article_from_json(article_folder_path, md_converter, output_folders, gemini_model):
    json_path = os.path.join(article_folder_path, 'data.json')
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        return {'status': 'error', 'title': os.path.basename(article_folder_path), 'reason': f"æ— æ³•è¯»å–JSON: {e}"}

    original_title = data['original_title']
    print(f"  -> å¼€å§‹ä¼˜åŒ–: {original_title[:50]}...")
    content_html = data['content_html']
    initial_md_for_meta = md_converter.handle(content_html)

    ai_results = None
    if gemini_model:
        text_snippet = re.sub(r'\s+', ' ', initial_md_for_meta).replace('#', '').replace('*', '').replace('-', '')[:500]
        ai_results = generate_seo_with_ai(gemini_model, original_title, text_snippet, data['author'])

    ai_used = False
    if ai_results:
        seo_title, description, tags = ai_results['seo_title'], ai_results['description'], ai_results['tags']
        ai_used = True
        tags.extend(["æ±‰åŒ–", "GALGAME"])
        if data['author'] != "æœªçŸ¥ä½œè€…": tags.append(data['author'])
        tags = sorted(list(set(tags)))
    else:
        seo_title = create_seo_title(original_title)
        description = create_meta_description(content_html, initial_md_for_meta)
        tags = extract_tags_from_title(original_title, data['author'])

    safe_filename = sanitize_filename(seo_title)
    if safe_filename in get_existing_processed_titles(FINAL_OUTPUT_PARENT_FOLDER):
        add_processed_article(article_folder_path)
        return {'status': 'skipped', 'title': seo_title}

    html_with_alts = add_alt_tags_to_html(content_html, seo_title)
    content_md = md_converter.handle(html_with_alts)
    
    front_matter_parts = ['---', f'title: "{original_title}"', f'seo_title: "{seo_title}"']
    if data.get("cover_image_url"): front_matter_parts.append(f'cover: "{data.get("cover_image_url")}"')
    front_matter_parts.extend([f'description: "{description}"', 'categories:', '  - èµ„æºè´´', 'tags:'] + [f'  - {tag}' for tag in tags] + [f'date: {data["hexo_date"]}', '---'])
    front_matter = "\n".join(front_matter_parts) + "\n\n"
    
    body_header = f"# {original_title}\n\n**{data['author']}** - {data['publish_date']}\n\n"
    source_footer = f"\n\n---\n\n**Source:** [{original_title}]({data['source_url']})\n"
    structured_data = generate_structured_data(seo_title, original_title, data.get('cover_image_url'), description, data['hexo_date'], data['author'], data['source_url'])
    
    final_content_remote = front_matter + body_header + content_md + source_footer + structured_data
    with open(os.path.join(output_folders['remote'], f"{safe_filename}.md"), 'w', encoding='utf-8') as f:
        f.write(final_content_remote)
    
    local_asset_folder = os.path.join(output_folders['local'], safe_filename)
    os.makedirs(local_asset_folder, exist_ok=True)
    soup_for_local = BeautifulSoup(content_html, 'html.parser')
    
    if not soup_for_local.find_all('img') or not os.path.exists(os.path.join(article_folder_path, 'images')):
        shutil.copy(os.path.join(output_folders['remote'], f"{safe_filename}.md"), os.path.join(output_folders['local'], f"{safe_filename}.md"))
    else:
        for i, img in enumerate(soup_for_local.find_all('img'), 1):
            original_src = img.get('src')
            if original_src and original_src.startswith('images/'):
                img_filename = os.path.basename(original_src)
                source_image_path = os.path.join(article_folder_path, 'images', img_filename)
                if os.path.exists(source_image_path):
                    shutil.copy(source_image_path, local_asset_folder)
                    alt_text = f'{seo_title} - å›¾{i}'.replace('"', '\\"')
                    img.replace_with(BeautifulSoup(f'{{% asset_img {img_filename} "{alt_text}" %}}', 'html.parser'))
        
        final_content_local = front_matter + body_header + md_converter.handle(str(soup_for_local)) + source_footer + structured_data
        with open(os.path.join(output_folders['local'], f"{safe_filename}.md"), 'w', encoding='utf-8') as f:
            f.write(final_content_local)
    
    add_processed_article(article_folder_path)
    return {'status': 'success_ai' if ai_used else 'success_local', 'title': seo_title}


def main():
    send_pushplus_notification("South-Plusä¼˜åŒ–å™¨ä»»åŠ¡å¼€å§‹", f"ä»»åŠ¡äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} å¯åŠ¨ã€‚")
    if not os.path.exists(RAW_DATA_PARENT_FOLDER):
        send_pushplus_notification("ä¼˜åŒ–å™¨ä»»åŠ¡è­¦å‘Š", f"'{RAW_DATA_PARENT_FOLDER}' ä¸å­˜åœ¨ï¼Œä»»åŠ¡è·³è¿‡ã€‚")
        return

    initialize_database()
    already_processed = get_processed_articles()

    gemini_model = None
    if not GEMINI_API_KEY or GEMINI_API_KEY == 'YOUR_GEMINI_API_KEY_HERE':
        send_pushplus_notification("ä¼˜åŒ–å™¨ä»»åŠ¡è­¦å‘Š", "GEMINI_API_KEY æœªé…ç½®ï¼Œå°†ä½¿ç”¨æœ¬åœ°è§„åˆ™ã€‚")
    else:
        try:
            genai.configure(api_key=GEMINI_API_KEY)
            gemini_model = genai.GenerativeModel('gemini-pro')
            send_pushplus_notification("Debug: AIçŠ¶æ€", "Geminiæ¨¡å‹å·²åŠ è½½ã€‚")
        except Exception as e:
            send_pushplus_notification("ä¼˜åŒ–å™¨ä»»åŠ¡ä¸¥é‡é”™è¯¯", f"Geminiåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æœ¬åœ°è§„åˆ™ã€‚")

    all_articles_to_process = []
    for date_folder in sorted(os.listdir(RAW_DATA_PARENT_FOLDER)):
        date_path = os.path.join(RAW_DATA_PARENT_FOLDER, date_folder)
        if os.path.isdir(date_path):
            for article_folder in os.listdir(date_path):
                article_path = os.path.join(date_path, article_folder)
                if os.path.isdir(article_path) and article_path not in already_processed:
                    all_articles_to_process.append({'path': article_path, 'date': date_folder})

    if not all_articles_to_process:
        send_pushplus_notification("ä¼˜åŒ–å™¨ä»»åŠ¡å®Œæˆ", "æ²¡æœ‰å‘ç°ä»»ä½•æ–°çš„åŸå§‹æ•°æ®éœ€è¦å¤„ç†ã€‚")
        return

    print(f"å‘ç° {len(all_articles_to_process)} ç¯‡æ–°æ–‡ç« å¾…ä¼˜åŒ–ã€‚")
    md_converter = html2text.HTML2Text(bodywidth=0)
    md_converter.links_each_paragraph = True
    
    processed_count = 0
    results = []
    batch_results = []
    
    for item in all_articles_to_process:
        processed_count += 1
        date_folder_name = item['date']
        article_path = item['path']
        output_remote = os.path.join(FINAL_OUTPUT_PARENT_FOLDER, date_folder_name)
        output_local = os.path.join(FINAL_OUTPUT_PARENT_FOLDER, f"{date_folder_name}_local")
        os.makedirs(output_remote, exist_ok=True)
        os.makedirs(output_local, exist_ok=True)
        output_folders = {'remote': output_remote, 'local': output_local}
        
        result = process_article_from_json(article_path, md_converter, output_folders, gemini_model)
        results.append(result)
        batch_results.append(result)

        if processed_count % REPORTING_BATCH_SIZE == 0 or processed_count == len(all_articles_to_process):
            ai = sum(1 for r in batch_results if r['status'] == 'success_ai')
            local = sum(1 for r in batch_results if r['status'] == 'success_local')
            error = sum(1 for r in batch_results if r['status'] == 'error')
            skipped = sum(1 for r in batch_results if r['status'] == 'skipped')
            summary = f"æ‰¹æ¬¡è¿›åº¦ ({processed_count}/{len(all_articles_to_process)}):\n- AIä¼˜åŒ–: {ai}\n- æœ¬åœ°è§„åˆ™: {local}\n- è·³è¿‡: {skipped}\n- å¤±è´¥: {error}"
            send_pushplus_notification(f"ä¼˜åŒ–å™¨è¿›åº¦æŠ¥å‘Š ({processed_count}/{len(all_articles_to_process)})", summary)
            batch_results = []

    ai_total = sum(1 for r in results if r['status'] == 'success_ai')
    local_total = sum(1 for r in results if r['status'] == 'success_local')
    error_total = sum(1 for r in results if r['status'] == 'error')
    skipped_total = sum(1 for r in results if r['status'] == 'skipped')
    final_summary = f"æ‰€æœ‰ä¼˜åŒ–ä»»åŠ¡å·²å®Œæˆã€‚\n\nä»»åŠ¡æ€»æ•°: {len(all_articles_to_process)}\n- ğŸ¤– AIä¼˜åŒ–æˆåŠŸ: {ai_total}\n- âš™ï¸ æœ¬åœ°è§„åˆ™æˆåŠŸ: {local_total}\n- â­ï¸ å·²å­˜åœ¨è·³è¿‡: {skipped_total}\n- âŒ å¤„ç†å¤±è´¥: {error_total}"
    send_pushplus_notification("ä¼˜åŒ–å™¨ä»»åŠ¡æœ€ç»ˆæ€»ç»“", final_summary)
    
if __name__ == "__main__":
    main()
